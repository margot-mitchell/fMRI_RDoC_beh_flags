"""Generates quality control flags by comparing metrics to thresholds.

This script reads the metrics CSV files generated by calculate_metrics.py
and compares them against task-specific thresholds to generate quality control flags.

Usage:
    python generate_flags.py <subject_folder>
    Example: python generate_flags.py sub-SK
"""

import os
import sys
import logging
from typing import List, Tuple
import importlib
import argparse

import polars as pl
import pandas as pd

# Dynamically load all uppercase threshold variables from thresholds_config.py
thresholds_module = importlib.import_module('thresholds_config')
THRESHOLDS = {k: getattr(thresholds_module, k) for k in dir(thresholds_module) if k.isupper()}

# Map file names to internal task names
TASK_NAME_MAP = {
    'stopSignal': 'stop_signal',
    'stopSignal_practice': 'stop_signal',
    'axCPT': 'ax_cpt',
    'axCPT_practice': 'ax_cpt',
    'goNogo': 'gonogo',
    'goNogo_practice': 'gonogo',
    'opSpan': 'operation_span',
    'opSpan_practice': 'operation_span',
    'opOnly': 'operation_only_span',
    'opOnly_practice': 'operation_only_span',
    'simpleSpan': 'simple_span',
    'simpleSpan_practice': 'simple_span',
    'nBack': 'nback',
    'nBack_practice': 'nback',
    'cuedTS': 'cued_ts',
    'cuedTS_practice': 'cued_ts',
    'spatialCueing': 'spatial_cueing',
    'spatialCueing_practice': 'spatial_cueing',
    'spatialTS': 'spatial_ts',
    'spatialTS_practice': 'spatial_ts',
    'stroop': 'stroop',
    'stroop_practice': 'stroop',
    'visualSearch': 'visual_search',
    'visualSearch_practice': 'visual_search',
    'flanker': 'flanker'
}

# Map metric names to their corresponding threshold variables
METRIC_TO_THRESHOLD = {
    # General metrics
    'proportion_feedback': 'PROPORTION_FEEDBACK_THRESHOLD',
    
    # Stop Signal
    'stop_signal_go_accuracy': 'STOP_SIGNAL_GO_ACCURACY',
    'stop_signal_go_rt': 'STOP_SIGNAL_GO_RT',
    'stop_signal_go_omission_rate': 'STOP_SIGNAL_OMISSION_RATE',
    'stop_signal_stop_accuracy': 'STOP_SIGNAL_STOP_ACCURACY_MIN',  # Will be handled specially in check_thresholds_from_csv
    
    # AX-CPT
    'ax_cpt_AX_accuracy': 'AX_CPT_AX_ACCURACY',
    'ax_cpt_BX_accuracy': 'AX_CPT_BX_ACCURACY',
    'ax_cpt_AY_accuracy': 'AX_CPT_AY_ACCURACY',
    'ax_cpt_BY_accuracy': 'AX_CPT_BY_ACCURACY',
    'ax_cpt_AX_omission_rate': 'AX_CPT_AX_OMISSION_RATE',
    'ax_cpt_BX_omission_rate': 'AX_CPT_BX_OMISSION_RATE',
    'ax_cpt_AY_omission_rate': 'AX_CPT_AY_OMISSION_RATE',
    'ax_cpt_BY_omission_rate': 'AX_CPT_BY_OMISSION_RATE',
    
    # Go/NoGo
    'gonogo_go_accuracy': 'GONOGO_GO_ACCURACY_MIN',
    'gonogo_nogo_accuracy': 'GONOGO_NOGO_ACCURACY_MIN',
    'gonogo_mean_accuracy': 'GONOGO_MEAN_ACCURACY',
    'gonogo_go_omission_rate': 'GONOGO_GO_OMISSION_RATE',
    'gonogo_go_rt': 'RT_THRESHOLD',
    
    # Operation Span
    '8x8_grid_asymmetric_accuracy': 'OP_SPAN_ASYMMETRIC_ACCURACY',
    '8x8_grid_symmetric_accuracy': 'OP_SPAN_SYMMETRIC_ACCURACY',
    'mean_4x4_grid_accuracy_irrespective_of_order': 'OP_SPAN_4X4_IRRESPECTIVE_ACCURACY',
    'mean_4x4_grid_accuracy_respective_of_order': 'OP_SPAN_ORDER_DIFF',
    
    # Operation Only Span
    '8x8_asymmetric_accuracy': 'OP_ONLY_SPAN_ASYMMETRIC_ACCURACY',
    '8x8_symmetric_accuracy': 'OP_ONLY_SPAN_SYMMETRIC_ACCURACY',
    
    # Simple Span
    'mean_4x4_grid_accuracy_irrespective_of_order': 'SIMPLE_SPAN_4X4_IRRESPECTIVE_ACCURACY',
    'mean_4x4_grid_accuracy_respective_of_order': 'SIMPLE_SPAN_ORDER_DIFF',
    
    # N-Back
    'weighted_2back_accuracy': 'NBACK_WEIGHTED_2BACK_ACCURACY',
    'weighted_1back_accuracy': 'NBACK_WEIGHTED_1BACK_ACCURACY',
    'match_1_omission_rate': 'NBACK_OMISSION_RATE',
    'match_2_omission_rate': 'NBACK_OMISSION_RATE',
    'mismatch_1_omission_rate': 'NBACK_OMISSION_RATE',
    'mismatch_2_omission_rate': 'NBACK_OMISSION_RATE',
    
    # Cued TS
    'switch_stay_accuracy': 'CUED_TS_SWITCH_STAY_ACCURACY',
    'stay_switch_accuracy': 'CUED_TS_STAY_SWITCH_ACCURACY',
    'switch_switch_accuracy': 'CUED_TS_SWITCH_SWITCH_ACCURACY',
    'switch_stay_omission_rate': 'CUED_TS_SWITCH_STAY_OMISSION_RATE',
    'stay_switch_omission_rate': 'CUED_TS_STAY_SWITCH_OMISSION_RATE',
    'switch_switch_omission_rate': 'CUED_TS_SWITCH_SWITCH_OMISSION_RATE',
    'parity_accuracy': 'CUED_TS_PARITY_ACCURACY',
    'magnitude_accuracy': 'CUED_TS_MAGNITUDE_ACCURACY',
    
    # Spatial Cueing
    'double_cue_accuracy': 'SPATIAL_CUEING_DOUBLE_CUE_ACCURACY',
    'double_cue_omission_rate': 'SPATIAL_CUEING_DOUBLE_CUE_OMISSION_RATE',
    'invalid_cue_accuracy': 'SPATIAL_CUEING_INVALID_CUE_ACCURACY',
    'invalid_cue_omission_rate': 'SPATIAL_CUEING_INVALID_CUE_OMISSION_RATE',
    'no_cue_accuracy': 'SPATIAL_CUEING_NO_CUE_ACCURACY',
    'no_cue_omission_rate': 'SPATIAL_CUEING_NO_CUE_OMISSION_RATE',
    'valid_cue_accuracy': 'SPATIAL_CUEING_VALID_CUE_ACCURACY',
    'valid_cue_omission_rate': 'SPATIAL_CUEING_VALID_CUE_OMISSION_RATE',
    
    # Spatial TS
    'stay_stay_accuracy': 'SPATIAL_TS_STAY_STAY_ACCURACY',
    'stay_stay_omission_rate': 'SPATIAL_TS_STAY_STAY_OMISSION_RATE',
    'stay_switch_accuracy': 'SPATIAL_TS_STAY_SWITCH_ACCURACY',
    'stay_switch_omission_rate': 'SPATIAL_TS_STAY_SWITCH_OMISSION_RATE',
    'switch_switch_accuracy': 'SPATIAL_TS_SWITCH_SWITCH_ACCURACY',
    'switch_switch_omission_rate': 'SPATIAL_TS_SWITCH_SWITCH_OMISSION_RATE',
    'color_accuracy': 'SPATIAL_TS_COLOR_ACCURACY',
    'form_accuracy': 'SPATIAL_TS_FORM_ACCURACY',
    
    # Stroop
    'congruent_accuracy': 'STROOP_CONGRUENT_ACCURACY',
    'congruent_omission_rate': 'STROOP_CONGRUENT_OMISSION_RATE',
    'incongruent_accuracy': 'STROOP_INCONGRUENT_ACCURACY',
    'incongruent_omission_rate': 'STROOP_INCONGRUENT_OMISSION_RATE',
    
    # Visual Search (new 24/8 naming)
    'conjunction_24_accuracy': 'VISUAL_SEARCH_CONJUNCTION_24_ACCURACY',
    'conjunction_8_accuracy': 'VISUAL_SEARCH_CONJUNCTION_8_ACCURACY',
    'feature_24_accuracy': 'VISUAL_SEARCH_FEATURE_24_ACCURACY',
    'feature_8_accuracy': 'VISUAL_SEARCH_FEATURE_8_ACCURACY',
    'conjunction_24_omission_rate': 'VISUAL_SEARCH_CONJUNCTION_24_OMISSION_RATE',
    'conjunction_8_omission_rate': 'VISUAL_SEARCH_CONJUNCTION_8_OMISSION_RATE',
    'feature_24_omission_rate': 'VISUAL_SEARCH_FEATURE_24_OMISSION_RATE',
    'feature_8_omission_rate': 'VISUAL_SEARCH_FEATURE_8_OMISSION_RATE',
    'conjunction_24_rt': 'VISUAL_SEARCH_CONJUNCTION_24_RT',
    'conjunction_8_rt': 'VISUAL_SEARCH_CONJUNCTION_8_RT',
    'feature_24_rt': 'VISUAL_SEARCH_FEATURE_24_RT',
    'feature_8_rt': 'VISUAL_SEARCH_FEATURE_8_RT',
    
    # Flanker
    'congruent_accuracy': 'FLANKER_ACCURACY_CONGRUENT_ACCURACY',
    'congruent_omission_rate': 'FLANKER_CONGRUENT_OMISSION_RATE',
    'incongruent_accuracy': 'FLANKER_INCONGRUENT_ACCURACY',
    'incongruent_omission_rate': 'FLANKER_INCONGRUENT_OMISSION_RATE'
}

def check_thresholds_from_csv(task_metrics_df: pl.DataFrame, task_name: str) -> List[Tuple[str, float, float]]:
    """Check if any metrics violate thresholds by reading from metrics CSV.
    
    Args:
        task_metrics_df (pl.DataFrame): DataFrame containing task metrics
        task_name (str): Name of the task
        
    Returns:
        List[Tuple[str, float, float]]: List of violations (metric, value, threshold)
    """
    violations = []
    
    # Convert task_metrics_df to dictionary for easier access
    metrics_dict = dict(zip(task_metrics_df['metric'], task_metrics_df['value']))
    
    for metric, value in metrics_dict.items():
        # Rename metrics to be task-specific for consistent mapping
        if metric == 'go_rt':
            if task_name == 'stop_signal':
                metric = 'stop_signal_go_rt'
            elif task_name == 'gonogo':
                metric = 'gonogo_go_rt'
        
        # Try to find a threshold variable for this metric
        threshold_var = METRIC_TO_THRESHOLD.get(metric)
        if not threshold_var:
            # Try to construct a threshold variable name from the metric and task
            # e.g., for 'go_accuracy' in stop_signal, try 'STOP_SIGNAL_GO_ACCURACY'
            metric_upper = metric.upper()
            task_upper = task_name.upper()
            threshold_var = f'{task_upper}_{metric_upper}'
            if threshold_var not in THRESHOLDS:
                # Try just the metric name
                threshold_var = metric_upper
        if threshold_var in THRESHOLDS:
            threshold = THRESHOLDS[threshold_var]
            if isinstance(threshold, (int, float)):
                # Special case for stop_accuracy in stop signal task
                if task_name == 'stop_signal' and metric == 'stop_accuracy':
                    min_threshold = THRESHOLDS['STOP_SIGNAL_STOP_ACCURACY_MIN']
                    max_threshold = THRESHOLDS['STOP_SIGNAL_STOP_ACCURACY_MAX']
                    if value is not None:
                        if value < min_threshold:
                            violations.append((metric, value, min_threshold))
                        elif value > max_threshold:
                            violations.append((metric, value, max_threshold))
                # Special case for proportion_feedback
                elif metric == 'proportion_feedback':
                    threshold = THRESHOLDS['PROPORTION_FEEDBACK_THRESHOLD']
                    if value is not None and value > threshold:
                        violations.append((metric, value, threshold))
                # For RT metrics, flag if value is ABOVE threshold (too slow)
                elif 'rt' in metric.lower():
                    if value is not None and value > threshold:
                        violations.append((metric, value, threshold))
                # For omission rates, flag if value is ABOVE threshold (maximum)
                elif 'omission_rate' in metric:
                    if value is not None and value > threshold:
                        violations.append((metric, value, threshold))
                # For all other metrics (like accuracy), flag if value is BELOW threshold (minimum)
                elif value is not None:
                    if value < threshold:
                        violations.append((metric, value, threshold))
        else:
            # If no specific threshold found, check for general thresholds
            if 'rt' in metric.lower():
                # Use general RT threshold for any RT metric without a specific threshold
                general_rt_threshold = THRESHOLDS['RT_THRESHOLD']
                if value is not None and value > general_rt_threshold:
                    violations.append((metric, value, general_rt_threshold))
    
    return violations

def calculate_nback_metrics(metrics_dict):
    """Calculate weighted accuracies and conditional accuracy comparisons for N-back tasks.
    
    Args:
        metrics_dict (dict): Dictionary of metrics from the N-back task
        
    Returns:
        dict: Dictionary with calculated N-back metrics
    """
    calculated_metrics = {}
    
    # Calculate weighted 2-back accuracy
    match_2_acc = metrics_dict.get('match_2_accuracy')
    mismatch_2_acc = metrics_dict.get('mismatch_2_accuracy')
    if match_2_acc is not None and mismatch_2_acc is not None:
        weighted_2back = (match_2_acc * THRESHOLDS['NBACK_MATCH_WEIGHT'] + 
                         mismatch_2_acc * THRESHOLDS['NBACK_MISMATCH_WEIGHT'])
        calculated_metrics['weighted_2back_accuracy'] = weighted_2back
        
        # Output actual values for comparison
        comparison_2back = f"match={match_2_acc}, mismatch={mismatch_2_acc}"
        calculated_metrics['match_and_mismatch__2back_accuracy_comparision'] = comparison_2back
    
    # Calculate weighted 1-back accuracy
    match_1_acc = metrics_dict.get('match_1_accuracy')
    mismatch_1_acc = metrics_dict.get('mismatch_1_accuracy')
    if match_1_acc is not None and mismatch_1_acc is not None:
        weighted_1back = (match_1_acc * THRESHOLDS['NBACK_MATCH_WEIGHT'] + 
                         mismatch_1_acc * THRESHOLDS['NBACK_MISMATCH_WEIGHT'])
        calculated_metrics['weighted_1back_accuracy'] = weighted_1back
        
        # Output actual values for comparison
        comparison_1back = f"match={match_1_acc}, mismatch={mismatch_1_acc}"
        calculated_metrics['match_and_mismatch__1back_accuracy_comparision'] = comparison_1back
    
    return calculated_metrics

def calculate_gonogo_metrics(metrics_dict):
    """Calculate mean accuracy for Go/NoGo task.
    
    Args:
        metrics_dict (dict): Dictionary of metrics from the Go/NoGo task
        
    Returns:
        dict: Dictionary with calculated Go/NoGo metrics
    """
    calculated_metrics = {}
    
    go_acc = metrics_dict.get('go_accuracy')
    nogo_acc = metrics_dict.get('nogo_accuracy')
    if go_acc is not None and nogo_acc is not None:
        mean_acc = (go_acc + nogo_acc) / 2
        calculated_metrics['mean_accuracy'] = mean_acc
    
    return calculated_metrics

def get_all_metrics_and_thresholds(task_name: str) -> List[Tuple[str, float]]:
    """Get all metrics and their thresholds for a given task.
    
    Args:
        task_name (str): Name of the task
        
    Returns:
        List[Tuple[str, float]]: List of (metric_name, threshold_value) pairs
    """
    metrics_thresholds = []
    
    # Add proportion_feedback for all tasks
    metrics_thresholds.append(('proportion_feedback', THRESHOLDS['PROPORTION_FEEDBACK_THRESHOLD']))
    
    # Note: RT metrics will be added dynamically when found in the actual data
    # The general RT_THRESHOLD will be applied to all metrics with "rt" in their name
    
    if task_name == 'nback':
        metrics_thresholds.extend([
            ('match_2_accuracy', THRESHOLDS['NBACK_MATCH_MIN_CONDITIONAL_ACCURACY']),
            ('mismatch_2_accuracy', THRESHOLDS['NBACK_MISMATCH_MIN_CONDITIONAL_ACCURACY']),
            ('match_1_accuracy', THRESHOLDS['NBACK_MATCH_MIN_CONDITIONAL_ACCURACY']),
            ('mismatch_1_accuracy', THRESHOLDS['NBACK_MISMATCH_MIN_CONDITIONAL_ACCURACY']),
            ('match_2_omission_rate', THRESHOLDS['NBACK_OMISSION_RATE']),
            ('mismatch_2_omission_rate', THRESHOLDS['NBACK_OMISSION_RATE']),
            ('match_1_omission_rate', THRESHOLDS['NBACK_OMISSION_RATE']),
            ('mismatch_1_omission_rate', THRESHOLDS['NBACK_OMISSION_RATE'])
        ])
    elif task_name == 'stop_signal':
        metrics_thresholds.extend([
            ('stop_accuracy', THRESHOLDS['STOP_SIGNAL_STOP_ACCURACY_MIN']),
            ('stop_accuracy', THRESHOLDS['STOP_SIGNAL_STOP_ACCURACY_MAX']),
            ('go_accuracy', THRESHOLDS['STOP_SIGNAL_GO_ACCURACY']),
            ('go_omission_rate', THRESHOLDS['STOP_SIGNAL_OMISSION_RATE']),
            ('stop_signal_go_rt', THRESHOLDS['STOP_SIGNAL_GO_RT'])
        ])
    elif task_name == 'ax_cpt':
        metrics_thresholds.extend([
            ('AX_accuracy', THRESHOLDS['AX_CPT_AX_ACCURACY']),
            ('AX_omission_rate', THRESHOLDS['AX_CPT_AX_OMISSION_RATE']),
            ('BX_accuracy', THRESHOLDS['AX_CPT_BX_ACCURACY']),
            ('BX_omission_rate', THRESHOLDS['AX_CPT_BX_OMISSION_RATE']),
            ('AY_accuracy', THRESHOLDS['AX_CPT_AY_ACCURACY']),
            ('AY_omission_rate', THRESHOLDS['AX_CPT_AY_OMISSION_RATE']),
            ('BY_accuracy', THRESHOLDS['AX_CPT_BY_ACCURACY']),
            ('BY_omission_rate', THRESHOLDS['AX_CPT_BY_OMISSION_RATE'])
        ])
    elif task_name == 'gonogo':
        metrics_thresholds.extend([
            ('go_accuracy', THRESHOLDS['GONOGO_GO_ACCURACY_MIN']),
            ('nogo_accuracy', THRESHOLDS['GONOGO_NOGO_ACCURACY_MIN']),
            ('go_omission_rate', THRESHOLDS['GONOGO_GO_OMISSION_RATE'])
        ])
    elif task_name == 'operation_span':
        metrics_thresholds.extend([
            ('8x8_grid_asymmetric_accuracy', THRESHOLDS['OP_SPAN_ASYMMETRIC_ACCURACY']),
            ('8x8_grid_symmetric_accuracy', THRESHOLDS['OP_SPAN_SYMMETRIC_ACCURACY']),
            ('mean_4x4_grid_accuracy_irrespective_of_order', THRESHOLDS['OP_SPAN_4X4_IRRESPECTIVE_ACCURACY']),
            ('mean_4x4_grid_accuracy_respective_of_order', THRESHOLDS['OP_SPAN_ORDER_DIFF'])
        ])
    elif task_name == 'operation_only_span':
        metrics_thresholds.extend([
            ('8x8_asymmetric_accuracy', THRESHOLDS['OP_ONLY_SPAN_ASYMMETRIC_ACCURACY']),
            ('8x8_symmetric_accuracy', THRESHOLDS['OP_ONLY_SPAN_SYMMETRIC_ACCURACY'])
        ])
    elif task_name == 'simple_span':
        metrics_thresholds.extend([
            ('mean_4x4_grid_accuracy_irrespective_of_order', THRESHOLDS['SIMPLE_SPAN_4X4_IRRESPECTIVE_ACCURACY']),
            ('mean_4x4_grid_accuracy_respective_of_order', THRESHOLDS['SIMPLE_SPAN_ORDER_DIFF'])
        ])
    elif task_name == 'cued_ts':
        metrics_thresholds.extend([
            ('task_stay_cue_stay_accuracy', THRESHOLDS['CUED_TS_SWITCH_STAY_ACCURACY']),
            ('task_stay_cue_stay_omission_rate', THRESHOLDS['CUED_TS_SWITCH_STAY_OMISSION_RATE']),
            ('task_stay_cue_switch_accuracy', THRESHOLDS['CUED_TS_STAY_SWITCH_ACCURACY']),
            ('task_stay_cue_switch_omission_rate', THRESHOLDS['CUED_TS_STAY_SWITCH_OMISSION_RATE']),
            ('task_switch_cue_switch_accuracy', THRESHOLDS['CUED_TS_SWITCH_SWITCH_ACCURACY']),
            ('task_switch_cue_switch_omission_rate', THRESHOLDS['CUED_TS_SWITCH_SWITCH_OMISSION_RATE']),
            ('parity_accuracy', THRESHOLDS['CUED_TS_PARITY_ACCURACY']),
            ('magnitude_accuracy', THRESHOLDS['CUED_TS_MAGNITUDE_ACCURACY'])
        ])
    elif task_name == 'spatial_cueing':
        metrics_thresholds.extend([
            ('doublecue_accuracy', THRESHOLDS['SPATIAL_CUEING_DOUBLE_CUE_ACCURACY']),
            ('doublecue_omission_rate', THRESHOLDS['SPATIAL_CUEING_DOUBLE_CUE_OMISSION_RATE']),
            ('invalid_accuracy', THRESHOLDS['SPATIAL_CUEING_INVALID_CUE_ACCURACY']),
            ('invalid_omission_rate', THRESHOLDS['SPATIAL_CUEING_INVALID_CUE_OMISSION_RATE']),
            ('nocue_accuracy', THRESHOLDS['SPATIAL_CUEING_NO_CUE_ACCURACY']),
            ('nocue_omission_rate', THRESHOLDS['SPATIAL_CUEING_NO_CUE_OMISSION_RATE']),
            ('valid_accuracy', THRESHOLDS['SPATIAL_CUEING_VALID_CUE_ACCURACY']),
            ('valid_omission_rate', THRESHOLDS['SPATIAL_CUEING_VALID_CUE_OMISSION_RATE'])
        ])
    elif task_name == 'spatial_ts':
        metrics_thresholds.extend([
            ('task_stay_cue_switch_accuracy', THRESHOLDS['SPATIAL_TS_STAY_SWITCH_ACCURACY']),
            ('task_stay_cue_switch_omission_rate', THRESHOLDS['SPATIAL_TS_STAY_SWITCH_OMISSION_RATE']),
            ('task_switch_cue_stay_accuracy', THRESHOLDS['SPATIAL_TS_STAY_STAY_ACCURACY']),
            ('task_switch_cue_stay_omission_rate', THRESHOLDS['SPATIAL_TS_STAY_STAY_OMISSION_RATE']),
            ('task_switch_cue_switch_accuracy', THRESHOLDS['SPATIAL_TS_SWITCH_SWITCH_ACCURACY']),
            ('task_switch_cue_switch_omission_rate', THRESHOLDS['SPATIAL_TS_SWITCH_SWITCH_OMISSION_RATE']),
            ('color_accuracy', THRESHOLDS['SPATIAL_TS_COLOR_ACCURACY']),
            ('form_accuracy', THRESHOLDS['SPATIAL_TS_FORM_ACCURACY'])
        ])
    elif task_name == 'stroop':
        for condition in ['congruent', 'incongruent']:
            metrics_thresholds.extend([
                (f'{condition}_accuracy', THRESHOLDS['STROOP_CONGRUENT_ACCURACY']),
                (f'{condition}_omission_rate', THRESHOLDS['STROOP_CONGRUENT_OMISSION_RATE'])
            ])
    elif task_name == 'visual_search':
        metrics_thresholds.extend([
            ('conjunction_24_accuracy', THRESHOLDS['VISUAL_SEARCH_CONJUNCTION_24_ACCURACY']),
            ('conjunction_24_omission_rate', THRESHOLDS['VISUAL_SEARCH_CONJUNCTION_24_OMISSION_RATE']),
            ('conjunction_8_accuracy', THRESHOLDS['VISUAL_SEARCH_CONJUNCTION_8_ACCURACY']),
            ('conjunction_8_omission_rate', THRESHOLDS['VISUAL_SEARCH_CONJUNCTION_8_OMISSION_RATE']),
            ('feature_24_accuracy', THRESHOLDS['VISUAL_SEARCH_FEATURE_24_ACCURACY']),
            ('feature_24_omission_rate', THRESHOLDS['VISUAL_SEARCH_FEATURE_24_OMISSION_RATE']),
            ('feature_8_accuracy', THRESHOLDS['VISUAL_SEARCH_FEATURE_8_ACCURACY']),
            ('feature_8_omission_rate', THRESHOLDS['VISUAL_SEARCH_FEATURE_8_OMISSION_RATE'])
        ])
    elif task_name == 'flanker':
        for condition in ['congruent', 'incongruent']:
            metrics_thresholds.extend([
                (f'{condition}_accuracy', THRESHOLDS['FLANKER_ACCURACY_CONGRUENT_ACCURACY']),
                (f'{condition}_omission_rate', THRESHOLDS['FLANKER_CONGRUENT_OMISSION_RATE'])
            ])
    
    return metrics_thresholds

def main():
    # Configure logging
    logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.WARNING)

    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Generate quality control flags')
    parser.add_argument('subject_folder', nargs='?', help='Subject folder to process (e.g., sub-SK)')
    parser.add_argument('--session', help='Specific session to process (e.g., ses-1, ses-pretouch)')
    args = parser.parse_args()

    if args.subject_folder:
        # Single subject mode
        subject_folders = [args.subject_folder]
    else:
        # All subjects mode
        metrics_dir = os.path.join('results', 'metrics')
        subject_folders = [f for f in os.listdir(metrics_dir)
                           if os.path.isdir(os.path.join(metrics_dir, f)) and f.startswith('sub-')]
        if not subject_folders:
            print(f"No subject folders found in {metrics_dir}")
            sys.exit(1)
        print(f"Processing all subjects: {', '.join(subject_folders)}")

    for subject_folder in subject_folders:
        print(f"\nProcessing subject folder: {subject_folder}")
        
        # Get all task directories for this subject
        metrics_dir = os.path.join('results', 'metrics', subject_folder)
        if not os.path.exists(metrics_dir):
            logging.error(f'Directory {metrics_dir} does not exist')
            continue

        # Get all task directories
        task_dirs = [d for d in os.listdir(metrics_dir)
                    if os.path.isdir(os.path.join(metrics_dir, d))]

        # Prepare output directory
        output_dir = os.path.join('results', 'flags', subject_folder)
        os.makedirs(output_dir, exist_ok=True)

        # Process each task
        for task_dir in task_dirs:
            print(f"  Processing task: {task_dir}")
            
            # Get all CSV files for this task
            task_path = os.path.join(metrics_dir, task_dir)
            csv_files = [f for f in os.listdir(task_path) if f.endswith('.csv')]

            if not csv_files:
                logging.warning(f'No CSV files found in {task_path}')
                continue

            # Filter files by session if specified
            if args.session:
                session_files = [f for f in csv_files if args.session in f]
                if not session_files:
                    logging.warning(f'No files found for session {args.session} in {task_path}')
                    continue
                csv_files = session_files
                print(f"    Processing {len(csv_files)} files for session {args.session}")

            task_flags_list = []
            
            # Process each metrics file
            for metrics_file in csv_files:
                # Get task name from file name (remove _metrics.csv)
                file_task_name = metrics_file.replace('_metrics.csv', '')
                task_name = TASK_NAME_MAP.get(file_task_name)
                
                if task_name is None:
                    logging.warning(f"Unknown task name for file {metrics_file}, skipping")
                    continue
                
                logging.debug(f"Processing {file_task_name} (internal name: {task_name})")
                
                # Read metrics from CSV
                metrics_path = os.path.join(task_path, metrics_file)
                task_metrics_df = pl.read_csv(metrics_path)
                logging.debug(f"Found {len(task_metrics_df)} metrics in {metrics_file}")
                
                # Check thresholds
                violations = check_thresholds_from_csv(task_metrics_df, task_name)
                if violations:
                    for metric, value, threshold in violations:
                        task_flags_list.append((task_name, metric, value, threshold))
                        logging.warning(f"Flag generated: {task_name} - {metric} = {value} (threshold: {threshold})")
            
            # Save flags if any were generated
            if task_flags_list:
                flags_df = pl.DataFrame(task_flags_list, schema=['task', 'metric', 'value', 'threshold'], orient="row")
                flags_path = os.path.join(output_dir, f'{task_dir}_flags.csv')
                flags_df.write_csv(flags_path)
                logging.warning(f"Saved {len(task_flags_list)} flags to {flags_path}")
            else:
                print(f"No flags found for {task_dir}")

if __name__ == '__main__':
    main() 