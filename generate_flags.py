"""Generates quality control flags by comparing metrics to thresholds.

This script reads the metrics CSV files generated by calculate_metrics.py
and compares them against task-specific thresholds to generate quality control flags.

Usage:
    python generate_flags.py <subject_folder>
    Example: python generate_flags.py sub-SK
"""

import os
import sys
import logging
from typing import List, Tuple
import importlib

import polars as pl
import pandas as pd

# Dynamically load all uppercase threshold variables from thresholds_config.py
thresholds_module = importlib.import_module('thresholds_config')
THRESHOLDS = {k: getattr(thresholds_module, k) for k in dir(thresholds_module) if k.isupper()}

# Map file names to internal task names
TASK_NAME_MAP = {
    'stopSignal': 'stop_signal',
    'axCPT': 'ax_cpt',
    'goNogo': 'gonogo',
    'opSpan': 'operation_span',
    'opOnly': 'operation_only_span',
    'simpleSpan': 'simple_span',
    'nBack': 'nback',
    'cuedTS': 'cued_ts',
    'spatialCueing': 'spatial_cueing',
    'spatialTS': 'spatial_ts',
    'stroop': 'stroop',
    'visualSearch': 'visual_search',
    'flanker': 'flanker'
}

# Map metric names to their corresponding threshold variables
METRIC_TO_THRESHOLD = {
    # Stop Signal
    'stop_signal_go_accuracy': 'STOP_SIGNAL_GO_ACCURACY',
    'stop_signal_go_rt': 'STOP_SIGNAL_GO_RT',
    'stop_signal_go_omission_rate': 'STOP_SIGNAL_OMISSION_RATE',
    'stop_signal_stop_accuracy': 'STOP_SIGNAL_STOP_ACCURACY_MIN',  # Will be handled specially in check_thresholds_from_csv
    
    # AX-CPT
    'ax_cpt_AX_accuracy': 'AX_CPT_AX_ACCURACY',
    'ax_cpt_BX_accuracy': 'AX_CPT_BX_ACCURACY',
    'ax_cpt_AY_accuracy': 'AX_CPT_AY_ACCURACY',
    'ax_cpt_BY_accuracy': 'AX_CPT_BY_ACCURACY',
    'ax_cpt_AX_omission_rate': 'AX_CPT_AX_OMISSION_RATE',
    'ax_cpt_BX_omission_rate': 'AX_CPT_BX_OMISSION_RATE',
    'ax_cpt_AY_omission_rate': 'AX_CPT_AY_OMISSION_RATE',
    'ax_cpt_BY_omission_rate': 'AX_CPT_BY_OMISSION_RATE',
    
    # Go/NoGo
    'gonogo_go_accuracy': 'GONOGO_GO_ACCURACY_MIN',
    'gonogo_nogo_accuracy': 'GONOGO_NOGO_ACCURACY_MIN',
    'gonogo_mean_accuracy': 'GONOGO_MEAN_ACCURACY',
    'gonogo_go_omission_rate': 'GONOGO_GO_OMISSION_RATE',
    
    # Operation Span
    '8x8_grid_asymmetric_accuracy': 'OP_SPAN_ASYMMETRIC_ACCURACY',
    '8x8_grid_symmetric_accuracy': 'OP_SPAN_SYMMETRIC_ACCURACY',
    'mean_4x4_grid_accuracy_irrespective_of_order': 'OP_SPAN_4X4_IRRESPECTIVE_ACCURACY',
    'mean_4x4_grid_accuracy_respective_of_order': 'OP_SPAN_ORDER_DIFF',
    
    # Operation Only Span
    '8x8_asymmetric_accuracy': 'OP_ONLY_SPAN_ASYMMETRIC_ACCURACY',
    '8x8_symmetric_accuracy': 'OP_ONLY_SPAN_SYMMETRIC_ACCURACY',
    
    # Simple Span
    'mean_4x4_grid_accuracy_irrespective_of_order': 'SIMPLE_SPAN_4X4_IRRESPECTIVE_ACCURACY',
    'mean_4x4_grid_accuracy_respective_of_order': 'SIMPLE_SPAN_ORDER_DIFF',
    
    # N-Back
    'weighted_2back_accuracy': 'NBACK_WEIGHTED_2BACK_ACCURACY',
    'weighted_1back_accuracy': 'NBACK_WEIGHTED_1BACK_ACCURACY',
    
    # Cued TS
    'switch_stay_accuracy': 'CUED_TS_SWITCH_STAY_ACCURACY',
    'stay_switch_accuracy': 'CUED_TS_STAY_SWITCH_ACCURACY',
    'switch_switch_accuracy': 'CUED_TS_SWITCH_SWITCH_ACCURACY',
    'switch_stay_omission_rate': 'CUED_TS_SWITCH_STAY_OMISSION_RATE',
    'stay_switch_omission_rate': 'CUED_TS_STAY_SWITCH_OMISSION_RATE',
    'switch_switch_omission_rate': 'CUED_TS_SWITCH_SWITCH_OMISSION_RATE',
    'parity_accuracy': 'CUED_TS_PARITY_ACCURACY',
    'magnitude_accuracy': 'CUED_TS_MAGNITUDE_ACCURACY',
    
    # Spatial Cueing
    'double_cue_accuracy': 'SPATIAL_CUEING_DOUBLE_CUE_ACCURACY',
    'double_cue_omission_rate': 'SPATIAL_CUEING_DOUBLE_CUE_OMISSION_RATE',
    'invalid_cue_accuracy': 'SPATIAL_CUEING_INVALID_CUE_ACCURACY',
    'invalid_cue_omission_rate': 'SPATIAL_CUEING_INVALID_CUE_OMISSION_RATE',
    'no_cue_accuracy': 'SPATIAL_CUEING_NO_CUE_ACCURACY',
    'no_cue_omission_rate': 'SPATIAL_CUEING_NO_CUE_OMISSION_RATE',
    'valid_cue_accuracy': 'SPATIAL_CUEING_VALID_CUE_ACCURACY',
    'valid_cue_omission_rate': 'SPATIAL_CUEING_VALID_CUE_OMISSION_RATE',
    
    # Spatial TS
    'stay_stay_accuracy': 'SPATIAL_TS_STAY_STAY_ACCURACY',
    'stay_stay_omission_rate': 'SPATIAL_TS_STAY_STAY_OMISSION_RATE',
    'stay_switch_accuracy': 'SPATIAL_TS_STAY_SWITCH_ACCURACY',
    'stay_switch_omission_rate': 'SPATIAL_TS_STAY_SWITCH_OMISSION_RATE',
    'switch_switch_accuracy': 'SPATIAL_TS_SWITCH_SWITCH_ACCURACY',
    'switch_switch_omission_rate': 'SPATIAL_TS_SWITCH_SWITCH_OMISSION_RATE',
    'color_accuracy': 'SPATIAL_TS_COLOR_ACCURACY',
    'form_accuracy': 'SPATIAL_TS_FORM_ACCURACY',
    
    # Stroop
    'congruent_accuracy': 'STROOP_CONGRUENT_ACCURACY',
    'congruent_omission_rate': 'STROOP_CONGRUENT_OMISSION_RATE',
    'incongruent_accuracy': 'STROOP_INCONGRUENT_ACCURACY',
    'incongruent_omission_rate': 'STROOP_INCONGRUENT_OMISSION_RATE',
    
    # Visual Search (new 24/8 naming)
    'conjunction_24_accuracy': 'VISUAL_SEARCH_CONJUNCTION_24_ACCURACY',
    'conjunction_8_accuracy': 'VISUAL_SEARCH_CONJUNCTION_8_ACCURACY',
    'feature_24_accuracy': 'VISUAL_SEARCH_FEATURE_24_ACCURACY',
    'feature_8_accuracy': 'VISUAL_SEARCH_FEATURE_8_ACCURACY',
    'conjunction_24_omission_rate': 'VISUAL_SEARCH_CONJUNCTION_24_OMISSION_RATE',
    'conjunction_8_omission_rate': 'VISUAL_SEARCH_CONJUNCTION_8_OMISSION_RATE',
    'feature_24_omission_rate': 'VISUAL_SEARCH_FEATURE_24_OMISSION_RATE',
    'feature_8_omission_rate': 'VISUAL_SEARCH_FEATURE_8_OMISSION_RATE',
    'conjunction_24_rt': 'VISUAL_SEARCH_CONJUNCTION_24_RT',
    'conjunction_8_rt': 'VISUAL_SEARCH_CONJUNCTION_8_RT',
    'feature_24_rt': 'VISUAL_SEARCH_FEATURE_24_RT',
    'feature_8_rt': 'VISUAL_SEARCH_FEATURE_8_RT',
    
    # Flanker
    'congruent_accuracy': 'FLANKER_ACCURACY_CONGRUENT_ACCURACY',
    'congruent_omission_rate': 'FLANKER_CONGRUENT_OMISSION_RATE',
    'incongruent_accuracy': 'FLANKER_INCONGRUENT_ACCURACY',
    'incongruent_omission_rate': 'FLANKER_INCONGRUENT_OMISSION_RATE'
}

def check_thresholds_from_csv(task_metrics_df: pl.DataFrame, task_name: str) -> List[Tuple[str, float, float]]:
    """Check if any metrics violate thresholds by reading from metrics CSV.
    
    Args:
        task_metrics_df (pl.DataFrame): DataFrame containing task metrics
        task_name (str): Name of the task
        
    Returns:
        List[Tuple[str, float, float]]: List of violations (metric, value, threshold)
    """
    violations = []
    
    # Convert task_metrics_df to dictionary for easier access
    metrics_dict = dict(zip(task_metrics_df['metric'], task_metrics_df['value']))
    
    for metric, value in metrics_dict.items():
        # Create task-specific metric name
        task_specific_metric = f"{task_name}_{metric}"
        
        # Try to find a threshold variable for this metric
        threshold_var = METRIC_TO_THRESHOLD.get(task_specific_metric)
        if not threshold_var:
            # Try to construct a threshold variable name from the metric and task
            # e.g., for 'go_accuracy' in stop_signal, try 'STOP_SIGNAL_GO_ACCURACY'
            metric_upper = metric.upper()
            task_upper = task_name.upper()
            threshold_var = f'{task_upper}_{metric_upper}'
            if threshold_var not in THRESHOLDS:
                # Try just the metric name
                threshold_var = metric_upper
        if threshold_var in THRESHOLDS:
            threshold = THRESHOLDS[threshold_var]
            if isinstance(threshold, (int, float)):
                # Special case for stop_accuracy in stop signal task
                if task_name == 'stop_signal' and metric == 'stop_accuracy':
                    min_threshold = THRESHOLDS['STOP_SIGNAL_STOP_ACCURACY_MIN']
                    max_threshold = THRESHOLDS['STOP_SIGNAL_STOP_ACCURACY_MAX']
                    if value is not None:
                        if value < min_threshold:
                            violations.append((metric, value, min_threshold))
                        elif value > max_threshold:
                            violations.append((metric, value, max_threshold))
                # For RT metrics, flag if value is ABOVE threshold (too slow)
                elif 'rt' in metric.lower():
                    if value is not None and value > threshold:
                        violations.append((metric, value, threshold))
                # For omission rates, flag if value is ABOVE threshold (maximum)
                elif 'omission_rate' in metric:
                    if value is not None and value > threshold:
                        violations.append((metric, value, threshold))
                # For all other metrics (like accuracy), flag if value is BELOW threshold (minimum)
                elif value is not None:
                    if value < threshold:
                        violations.append((metric, value, threshold))
    
    return violations

def calculate_nback_metrics(metrics_dict):
    """Calculate weighted accuracies and conditional accuracy comparisons for N-back tasks.
    
    Args:
        metrics_dict (dict): Dictionary of metrics from the N-back task
        
    Returns:
        dict: Dictionary with calculated N-back metrics
    """
    calculated_metrics = {}
    
    # Calculate weighted 2-back accuracy
    match_2_acc = metrics_dict.get('match_2_accuracy')
    mismatch_2_acc = metrics_dict.get('mismatch_2_accuracy')
    if match_2_acc is not None and mismatch_2_acc is not None:
        weighted_2back = (match_2_acc * THRESHOLDS['NBACK_MATCH_WEIGHT'] + 
                         mismatch_2_acc * THRESHOLDS['NBACK_MISMATCH_WEIGHT'])
        calculated_metrics['weighted_2back_accuracy'] = weighted_2back
        
        # Output actual values for comparison
        comparison_2back = f"match={match_2_acc}, mismatch={mismatch_2_acc}"
        calculated_metrics['match_and_mismatch__2back_accuracy_comparision'] = comparison_2back
    
    # Calculate weighted 1-back accuracy
    match_1_acc = metrics_dict.get('match_1_accuracy')
    mismatch_1_acc = metrics_dict.get('mismatch_1_accuracy')
    if match_1_acc is not None and mismatch_1_acc is not None:
        weighted_1back = (match_1_acc * THRESHOLDS['NBACK_MATCH_WEIGHT'] + 
                         mismatch_1_acc * THRESHOLDS['NBACK_MISMATCH_WEIGHT'])
        calculated_metrics['weighted_1back_accuracy'] = weighted_1back
        
        # Output actual values for comparison
        comparison_1back = f"match={match_1_acc}, mismatch={mismatch_1_acc}"
        calculated_metrics['match_and_mismatch__1back_accuracy_comparision'] = comparison_1back
    
    return calculated_metrics

def calculate_gonogo_metrics(metrics_dict):
    """Calculate mean accuracy for Go/NoGo task.
    
    Args:
        metrics_dict (dict): Dictionary of metrics from the Go/NoGo task
        
    Returns:
        dict: Dictionary with calculated Go/NoGo metrics
    """
    calculated_metrics = {}
    
    go_acc = metrics_dict.get('go_accuracy')
    nogo_acc = metrics_dict.get('nogo_accuracy')
    if go_acc is not None and nogo_acc is not None:
        mean_acc = (go_acc + nogo_acc) / 2
        calculated_metrics['mean_accuracy'] = mean_acc
    
    return calculated_metrics

def get_all_metrics_and_thresholds(task_name: str) -> List[Tuple[str, float]]:
    """Get all metrics and their thresholds for a given task.
    
    Args:
        task_name (str): Name of the task
        
    Returns:
        List[Tuple[str, float]]: List of (metric_name, threshold_value) pairs
    """
    metrics_thresholds = []
    
    if task_name == 'nback':
        metrics_thresholds.extend([
            ('match_2_accuracy', THRESHOLDS['NBACK_MATCH_MIN_CONDITIONAL_ACCURACY']),
            ('mismatch_2_accuracy', THRESHOLDS['NBACK_MISMATCH_MIN_CONDITIONAL_ACCURACY']),
            ('match_1_accuracy', THRESHOLDS['NBACK_MATCH_MIN_CONDITIONAL_ACCURACY']),
            ('mismatch_1_accuracy', THRESHOLDS['NBACK_MISMATCH_MIN_CONDITIONAL_ACCURACY'])
        ])
    elif task_name == 'stop_signal':
        metrics_thresholds.extend([
            ('stop_accuracy', THRESHOLDS['STOP_SIGNAL_STOP_ACCURACY_MIN']),
            ('stop_accuracy', THRESHOLDS['STOP_SIGNAL_STOP_ACCURACY_MAX']),
            ('go_accuracy', THRESHOLDS['STOP_SIGNAL_GO_ACCURACY']),
            ('go_rt', THRESHOLDS['STOP_SIGNAL_GO_RT']),
            ('go_omission_rate', THRESHOLDS['STOP_SIGNAL_OMISSION_RATE'])
        ])
    elif task_name == 'ax_cpt':
        for condition in ['AX', 'BX', 'AY', 'BY']:
            metrics_thresholds.extend([
                (f'{condition}_accuracy', THRESHOLDS['AX_CPT_AX_ACCURACY']),
                (f'{condition}_omission_rate', THRESHOLDS['AX_CPT_AX_OMISSION_RATE'])
            ])
    elif task_name == 'gonogo':
        metrics_thresholds.extend([
            ('go_accuracy', THRESHOLDS['GONOGO_GO_ACCURACY_MIN']),
            ('nogo_accuracy', THRESHOLDS['GONOGO_NOGO_ACCURACY_MIN']),
            ('go_omission_rate', THRESHOLDS['GONOGO_GO_OMISSION_RATE'])
        ])
    elif task_name == 'operation_span':
        metrics_thresholds.extend([
            ('8x8_grid_asymmetric_accuracy', THRESHOLDS['OP_SPAN_ASYMMETRIC_ACCURACY']),
            ('8x8_grid_symmetric_accuracy', THRESHOLDS['OP_SPAN_SYMMETRIC_ACCURACY']),
            ('mean_4x4_grid_accuracy_irrespective_of_order', THRESHOLDS['OP_SPAN_4X4_IRRESPECTIVE_ACCURACY']),
            ('mean_4x4_grid_accuracy_respective_of_order', THRESHOLDS['OP_SPAN_ORDER_DIFF'])
        ])
    elif task_name == 'operation_only_span':
        metrics_thresholds.extend([
            ('8x8_asymmetric_accuracy', THRESHOLDS['OP_ONLY_SPAN_ASYMMETRIC_ACCURACY']),
            ('8x8_symmetric_accuracy', THRESHOLDS['OP_ONLY_SPAN_SYMMETRIC_ACCURACY'])
        ])
    elif task_name == 'simple_span':
        metrics_thresholds.extend([
            ('mean_4x4_grid_accuracy_irrespective_of_order', THRESHOLDS['SIMPLE_SPAN_4X4_IRRESPECTIVE_ACCURACY']),
            ('mean_4x4_grid_accuracy_respective_of_order', THRESHOLDS['SIMPLE_SPAN_ORDER_DIFF'])
        ])
    elif task_name == 'cued_ts':
        metrics_thresholds.extend([
            ('task_stay_cue_stay_accuracy', THRESHOLDS['CUED_TS_SWITCH_STAY_ACCURACY']),
            ('task_stay_cue_stay_omission_rate', THRESHOLDS['CUED_TS_SWITCH_STAY_OMISSION_RATE']),
            ('task_stay_cue_switch_accuracy', THRESHOLDS['CUED_TS_STAY_SWITCH_ACCURACY']),
            ('task_stay_cue_switch_omission_rate', THRESHOLDS['CUED_TS_STAY_SWITCH_OMISSION_RATE']),
            ('task_switch_cue_switch_accuracy', THRESHOLDS['CUED_TS_SWITCH_SWITCH_ACCURACY']),
            ('task_switch_cue_switch_omission_rate', THRESHOLDS['CUED_TS_SWITCH_SWITCH_OMISSION_RATE']),
            ('parity_accuracy', THRESHOLDS['CUED_TS_PARITY_ACCURACY']),
            ('magnitude_accuracy', THRESHOLDS['CUED_TS_MAGNITUDE_ACCURACY'])
        ])
    elif task_name == 'spatial_cueing':
        metrics_thresholds.extend([
            ('doublecue_accuracy', THRESHOLDS['SPATIAL_CUEING_DOUBLE_CUE_ACCURACY']),
            ('doublecue_omission_rate', THRESHOLDS['SPATIAL_CUEING_DOUBLE_CUE_OMISSION_RATE']),
            ('invalid_accuracy', THRESHOLDS['SPATIAL_CUEING_INVALID_CUE_ACCURACY']),
            ('invalid_omission_rate', THRESHOLDS['SPATIAL_CUEING_INVALID_CUE_OMISSION_RATE']),
            ('nocue_accuracy', THRESHOLDS['SPATIAL_CUEING_NO_CUE_ACCURACY']),
            ('nocue_omission_rate', THRESHOLDS['SPATIAL_CUEING_NO_CUE_OMISSION_RATE']),
            ('valid_accuracy', THRESHOLDS['SPATIAL_CUEING_VALID_CUE_ACCURACY']),
            ('valid_omission_rate', THRESHOLDS['SPATIAL_CUEING_VALID_CUE_OMISSION_RATE'])
        ])
    elif task_name == 'spatial_ts':
        metrics_thresholds.extend([
            ('task_stay_cue_switch_accuracy', THRESHOLDS['SPATIAL_TS_STAY_SWITCH_ACCURACY']),
            ('task_stay_cue_switch_omission_rate', THRESHOLDS['SPATIAL_TS_STAY_SWITCH_OMISSION_RATE']),
            ('task_switch_cue_stay_accuracy', THRESHOLDS['SPATIAL_TS_STAY_STAY_ACCURACY']),
            ('task_switch_cue_stay_omission_rate', THRESHOLDS['SPATIAL_TS_STAY_STAY_OMISSION_RATE']),
            ('task_switch_cue_switch_accuracy', THRESHOLDS['SPATIAL_TS_SWITCH_SWITCH_ACCURACY']),
            ('task_switch_cue_switch_omission_rate', THRESHOLDS['SPATIAL_TS_SWITCH_SWITCH_OMISSION_RATE']),
            ('color_accuracy', THRESHOLDS['SPATIAL_TS_COLOR_ACCURACY']),
            ('form_accuracy', THRESHOLDS['SPATIAL_TS_FORM_ACCURACY'])
        ])
    elif task_name == 'stroop':
        for condition in ['congruent', 'incongruent']:
            metrics_thresholds.extend([
                (f'{condition}_accuracy', THRESHOLDS['STROOP_CONGRUENT_ACCURACY']),
                (f'{condition}_omission_rate', THRESHOLDS['STROOP_CONGRUENT_OMISSION_RATE'])
            ])
    elif task_name == 'visual_search':
        metrics_thresholds.extend([
            ('conjunction_24_accuracy', THRESHOLDS['VISUAL_SEARCH_CONJUNCTION_24_ACCURACY']),
            ('conjunction_24_omission_rate', THRESHOLDS['VISUAL_SEARCH_CONJUNCTION_24_OMISSION_RATE']),
            ('conjunction_8_accuracy', THRESHOLDS['VISUAL_SEARCH_CONJUNCTION_8_ACCURACY']),
            ('conjunction_8_omission_rate', THRESHOLDS['VISUAL_SEARCH_CONJUNCTION_8_OMISSION_RATE']),
            ('feature_24_accuracy', THRESHOLDS['VISUAL_SEARCH_FEATURE_24_ACCURACY']),
            ('feature_24_omission_rate', THRESHOLDS['VISUAL_SEARCH_FEATURE_24_OMISSION_RATE']),
            ('feature_8_accuracy', THRESHOLDS['VISUAL_SEARCH_FEATURE_8_ACCURACY']),
            ('feature_8_omission_rate', THRESHOLDS['VISUAL_SEARCH_FEATURE_8_OMISSION_RATE'])
        ])
    elif task_name == 'flanker':
        for condition in ['congruent', 'incongruent']:
            metrics_thresholds.extend([
                (f'{condition}_accuracy', THRESHOLDS['FLANKER_ACCURACY_CONGRUENT_ACCURACY']),
                (f'{condition}_omission_rate', THRESHOLDS['FLANKER_CONGRUENT_OMISSION_RATE'])
            ])
    
    return metrics_thresholds

def main():
    # Get subject folder from command line argument
    if len(sys.argv) != 2:
        print("Usage: python generate_flags.py <subject_folder>")
        sys.exit(1)
        
    subject_folder = sys.argv[1]
    print(f"Processing subject folder: {subject_folder}")
    
    # Set up logging
    logging.basicConfig(level=logging.WARNING)
    logger = logging.getLogger(__name__)
    
    # Get all metrics files in the output directory
    output_dir = os.path.join('outputs', subject_folder)
    metrics_dir = os.path.join(output_dir, 'metrics')
    metrics_files = [f for f in os.listdir(metrics_dir) if f.endswith('_metrics.csv')]
    logger.info(f"Found {len(metrics_files)} metrics files: {metrics_files}")
    
    all_flags = []
    all_metrics = []
    
    # Process each metrics file
    for metrics_file in metrics_files:
        # Get task name from file name (remove _metrics.csv)
        file_task_name = metrics_file.replace('_metrics.csv', '')
        task_name = TASK_NAME_MAP.get(file_task_name)
        
        if task_name is None:
            logger.warning(f"Unknown task name for file {metrics_file}, skipping")
            continue
            
        logger.debug(f"Processing {file_task_name} (internal name: {task_name})")
        
        # Read metrics from CSV
        metrics_path = os.path.join(metrics_dir, metrics_file)
        task_metrics_df = pl.read_csv(metrics_path)
        logger.debug(f"Found {len(task_metrics_df)} metrics in {metrics_file}")
        
        # Check thresholds
        violations = check_thresholds_from_csv(task_metrics_df, task_name)
        if violations:
            for metric, value, threshold in violations:
                all_flags.append((task_name, metric, value, threshold))
                logger.warning(f"Flag generated: {task_name} - {metric} = {value} (threshold: {threshold})")
        
        # Get all metrics and thresholds for this task
        metrics_thresholds = get_all_metrics_and_thresholds(task_name)
        
        # First, collect all the raw metrics
        raw_metrics = {}
        for metric_name, threshold in metrics_thresholds:
            if metric_name != 'order_difference' and threshold is not None:
                # Map visual search metrics from present/absent to 24/8 convention
                if task_name == 'visual_search':
                    if metric_name == 'conjunction_24_accuracy':
                        metric_name = 'conjunction_24_accuracy'
                    elif metric_name == 'conjunction_24_omission_rate':
                        metric_name = 'conjunction_24_omission_rate'
                    elif metric_name == 'conjunction_8_accuracy':
                        metric_name = 'conjunction_8_accuracy'
                    elif metric_name == 'conjunction_8_omission_rate':
                        metric_name = 'conjunction_8_omission_rate'
                    elif metric_name == 'feature_24_accuracy':
                        metric_name = 'feature_24_accuracy'
                    elif metric_name == 'feature_24_omission_rate':
                        metric_name = 'feature_24_omission_rate'
                    elif metric_name == 'feature_8_accuracy':
                        metric_name = 'feature_8_accuracy'
                    elif metric_name == 'feature_8_omission_rate':
                        metric_name = 'feature_8_omission_rate'
                
                filtered_df = task_metrics_df.filter(pl.col('metric') == metric_name)
                if len(filtered_df) > 0:
                    value = filtered_df['value'].item()
                    if pd.notna(value):
                        # Map back to 24/8 convention for storage
                        if task_name == 'visual_search':
                            if metric_name == 'conjunction_24_accuracy':
                                metric_name = 'conjunction_24_accuracy'
                            elif metric_name == 'conjunction_24_omission_rate':
                                metric_name = 'conjunction_24_omission_rate'
                            elif metric_name == 'conjunction_8_accuracy':
                                metric_name = 'conjunction_8_accuracy'
                            elif metric_name == 'conjunction_8_omission_rate':
                                metric_name = 'conjunction_8_omission_rate'
                            elif metric_name == 'feature_24_accuracy':
                                metric_name = 'feature_24_accuracy'
                            elif metric_name == 'feature_24_omission_rate':
                                metric_name = 'feature_24_omission_rate'
                            elif metric_name == 'feature_8_accuracy':
                                metric_name = 'feature_8_accuracy'
                            elif metric_name == 'feature_8_omission_rate':
                                metric_name = 'feature_8_omission_rate'
                        
                        raw_metrics[metric_name] = value
                        # Only add to all_metrics if it's not an N-back metric
                        if not metric_name.startswith('nback_') and not metric_name.startswith('match_and_mismatch__'):
                            all_metrics.append((task_name, metric_name, value, threshold))
                            logger.debug(f"Added metric: {task_name} - {metric_name} = {value} (threshold: {threshold})")
                else:
                    logger.warning(f"Metric {metric_name} not found in {metrics_file}")
        
        # Calculate Go/NoGo specific metrics if this is a Go/NoGo task
        if task_name == 'gonogo':
            # First collect the raw accuracies
            for metric in ['go_accuracy', 'nogo_accuracy', 'go_omission_rate']:
                filtered_df = task_metrics_df.filter(pl.col('metric') == metric)
                if len(filtered_df) > 0:
                    value = filtered_df['value'].item()
                    if pd.notna(value):
                        raw_metrics[metric] = value
            
            # Now calculate the derived metrics
            gonogo_metrics = calculate_gonogo_metrics(raw_metrics)
            for metric_name, value in gonogo_metrics.items():
                threshold = next((t for m, t in metrics_thresholds if m == metric_name), None)
                all_metrics.append((task_name, metric_name, value, threshold))
                logger.debug(f"Added calculated Go/NoGo metric: {task_name} - {metric_name} = {value} (threshold: {threshold})")
        
        # Calculate N-back specific metrics if this is an N-back task
        elif task_name == 'nback':
            # First collect the raw match and mismatch accuracies
            for metric in ['match_2_accuracy', 'mismatch_2_accuracy', 'match_1_accuracy', 'mismatch_1_accuracy']:
                filtered_df = task_metrics_df.filter(pl.col('metric') == metric)
                if len(filtered_df) > 0:
                    value = filtered_df['value'].item()
                    if pd.notna(value):
                        raw_metrics[metric] = value
            
            # Now calculate the derived metrics
            nback_metrics = calculate_nback_metrics(raw_metrics)
            for metric_name, value in nback_metrics.items():
                threshold = next((t for m, t in metrics_thresholds if m == metric_name), None)
                all_metrics.append((task_name, metric_name, value, threshold))
                logger.debug(f"Added calculated N-back metric: {task_name} - {metric_name} = {value} (threshold: {threshold})")
        
        # Calculate order difference for span tasks
        if task_name in ['operation_span', 'simple_span']:
            irrespective_acc = raw_metrics.get('mean_4x4_grid_accuracy_irrespective_of_order')
            respective_acc = raw_metrics.get('mean_4x4_grid_accuracy_respective_of_order')
            
            if irrespective_acc is not None and respective_acc is not None and respective_acc != 0:
                order_diff = (irrespective_acc - respective_acc) / respective_acc
                threshold = THRESHOLDS['OP_SPAN_ORDER_DIFF'] if task_name == 'operation_span' else THRESHOLDS['SIMPLE_SPAN_ORDER_DIFF']
                all_metrics.append((task_name, 'order_difference', order_diff, threshold))
                logger.debug(f"Added calculated metric: {task_name} - order_difference = {order_diff} (threshold: {threshold})")
            else:
                logger.warning(f"Could not calculate order difference for {task_name} - missing or invalid values")
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Save all metrics and their thresholds
    metrics_df = pl.DataFrame(all_metrics, schema=['task', 'metric', 'value', 'threshold'], orient="row")
    metrics_path = os.path.join(output_dir, 'all_metrics_checked.csv')
    metrics_df.write_csv(metrics_path)
    print(f"All task metrics checked and saved to {metrics_path}")
    
    # Save flags if any were generated
    if all_flags:
        flags_df = pl.DataFrame(all_flags, schema=['task', 'metric', 'value', 'threshold'], orient="row")
        flags_path = os.path.join(output_dir, 'flags.csv')
        flags_df.write_csv(flags_path)
        logger.warning(f"Saved {len(all_flags)} flags to {flags_path}")
    else:
        print(f"No flags found for {subject_folder}")
        
        # Verify the file was created and has content
        if os.path.exists(metrics_path):
            file_size = os.path.getsize(metrics_path)
            logger.debug(f"Output file size: {file_size} bytes")
            if file_size == 0:
                logger.error("Output file is empty!")
            else:
                # Read back the file to verify contents
                df = pl.read_csv(metrics_path)
                logger.debug(f"File contains {len(df)} rows")
        else:
            logger.error("Output file was not created!")

if __name__ == '__main__':
    main() 